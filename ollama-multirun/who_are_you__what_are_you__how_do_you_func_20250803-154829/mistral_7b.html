  <!DOCTYPE html>
  <html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    a:hover { background-color: yellow; color: black; }
    body { font-family: monospace; }
    header, footer { background-color: #f0f0f0; padding: 10px; }
    li { margin: 5px; }
    table, td, th { border-collapse: collapse; }
    td, th { border: 1px solid #cccccc; padding: 5px; text-align: right; }
    tr:hover { background-color: lightyellow; color: black; }
    textarea { border: 1px solid #cccccc; white-space: pre-wrap; width: 90%; }
    .box { display: inline-block; margin: 3px; padding: 2px; vertical-align: top; }
    .left { text-align: left; }
    .menu { font-size: small; }
  </style>
<title>ollama-multirun: mistral:7b</title></head><body>
<header><a href='../index.html'>ollama-multirun</a>: <a href='./index.html'>who_are_you__what_are_you__how_do_you_func</a>: <b>mistral:7b</b>: 20250803-154829<br /><br />
<span class='menu'>
<a href='models.html'>models</a>: 
<a href='./deepseek_r1_8b.html'>deepseek-r1:8b</a> 
<a href='./dolphin3_8b.html'>dolphin3:8b</a> 
<a href='./gemma3n_e4b.html'>gemma3n:e4b</a> 
<b>mistral:7b</b> 
<a href='./qwen2_5vl_7b.html'>qwen2.5vl:7b</a> 
<a href='./qwen3_8b.html'>qwen3:8b</a> 
</span>
</header>
<p>Prompt: (<a href='./prompt.txt'>raw</a>) (<a href='./who_are_you__what_are_you__how_do_you_func.prompt.yaml'>yaml</a>)
  words:15  bytes:69<br />
<textarea readonly rows='3'>Who are you, What are you, How do you function, and What can you do?</textarea>
</p>
<p>Output: mistral:7b (<a href='./mistral_7b.output.txt'>raw</a>)<br />
<textarea readonly rows='8'> I am a Large Language Model trained by Mistral AI. I am designed to generate human-like responses to text input and complete tasks that can be performed with the help of my training data.

My primary function is to assist users by providing information, answering questions, helping with writing and editing, and much more. I achieve this through understanding and generating coherent and contextually relevant responses based on the vast amount of data I was trained on.

I can perform various tasks such as answering questions, telling stories, summarizing texts, translating languages, providing explanations, and even helping with coding problems or homework assignments to some extent. However, my capabilities are bound by my training data and I don&#39;t have personal experiences or emotions like a human does.</textarea>
</p>
<div class='box'><table>
<tr><td class='left' colspan='2'>Stats (<a href='./mistral_7b.stats.txt'>raw</a>)</td></tr>
<tr><td class='left'>Words</td><td>127</td></tr>
<tr><td class='left'>Bytes</td><td>817</td></tr>
<tr><td class='left'>Total duration</td><td>7.745086s</td></tr>
<tr><td class='left'>Load duration</td><td>2.3243204s</td></tr>
<tr><td class='left'>Prompt eval count</td><td>24 token(s)</td></tr>
<tr><td class='left'>Prompt eval duration</td><td>152.8621ms</td></tr>
<tr><td class='left'>Prompt eval rate</td><td>157.00 tokens/s</td></tr>
<tr><td class='left'>Eval count</td><td>157 token(s)</td></tr>
<tr><td class='left'>Eval duration</td><td>5.267333s</td></tr>
<tr><td class='left'>Eval rate</td><td>29.81 tokens/s</td></tr>
</table></div>
<div class='box'><table>
<tr><td class='left' colspan='2'>Model (<a href='./mistral_7b.info.txt'>raw</a>)</td></tr>
<tr><td class='left'>Name</td><td class='left'><a href='../models.html#mistral_7b'>mistral:7b</a></td></tr>
<tr><td class='left'>Architecture</td><td class='left'>llama</td></tr>
<tr><td class='left'>Size</td><td class='left'>8.3 GB</td></tr>
<tr><td class='left'>Parameters</td><td class='left'>7.2B</td></tr>
<tr><td class='left'>Context length</td><td class='left'>32768</td></tr>
<tr><td class='left'>Embedding length</td><td  class='left'>4096</td></tr>
<tr><td class='left'>Quantization</td><td class='left'>Q4_K_M</td></tr>
<tr><td class='left'>Capabilities</td><td class='left'>completion<br />tools<br /></td>
</table></div>
<div class='box'><table>
<tr><td class='left' colspan='2'>System</td></tr>
<tr><td class='left'>Ollama proc</td><td class='left'>11%/89% CPU/GPU</td></tr>
<tr><td class='left'>Ollama context</td><td class='left'>16384</td></tr>
<tr><td class='left'>Ollama version</td><td class='left'>0.10.1</td></tr>
<tr><td class='left'>Multirun timeout</td><td class='left'>300 seconds</td></tr>
<tr><td class='left'>Sys arch</td><td class='left'>x86_64</td></tr>
<tr><td class='left'>Sys processor</td><td class='left'>unknown</td></tr>
<tr><td class='left'>sys memory</td><td class='left'>10G + 20G</td></tr>
<tr><td class='left'>Sys OS</td><td class='left'>CYGWIN_NT-10.0-22631 3.6.4-1.x86_64</td></tr>
</table></div>
<br /><br />
<footer>
<p><a href='../index.html'>ollama-multirun</a>: <a href='./index.html'>who_are_you__what_are_you__how_do_you_func</a>: <b>mistral:7b</b>: 20250803-154829</p>
<p>Page created: 2025-08-03 15:53:10</p>
<p>Generated with: <a target='ollama-multirun' href='https://github.com/attogram/ollama-multirun'>ollama-multirun</a> v5.20.3</p>
</footer></body></html>
