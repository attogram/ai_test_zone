  <!DOCTYPE html>
  <html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    a:hover { background-color: yellow; color: black; }
    body { font-family: monospace; }
    header, footer { background-color: #f0f0f0; padding: 10px; }
    li { margin: 5px; }
    table, td, th { border-collapse: collapse; }
    td, th { border: 1px solid #cccccc; padding: 5px; text-align: right; }
    tr:hover { background-color: lightyellow; color: black; }
    textarea { border: 1px solid #cccccc; white-space: pre-wrap; width: 90%; }
    .box { display: inline-block; margin: 3px; padding: 2px; vertical-align: top; }
    .left { text-align: left; }
    .menu { font-size: small; }
  </style>
<title>ollama-multirun: llava:7b</title></head><body>
<header><a href='../index.html'>ollama-multirun</a>: <a href='./index.html'>describe_this_image____docs_logo_logo_250_jpg</a>: <b>llava:7b</b>: 20250615-224247<br /><br />
<span class='menu'>
<a href='models.html'>models</a>: 
<a href='./granite3_2_vision_2b.html'>granite3.2-vision:2b</a> 
<a href='./minicpm_v_8b.html'>minicpm-v:8b</a> 
<a href='./qwen2_5vl_3b.html'>qwen2.5vl:3b</a> 
<a href='./qwen2_5vl_7b.html'>qwen2.5vl:7b</a> 
<a href='./gemma3_4b.html'>gemma3:4b</a> 
<b>llava:7b</b> 
<a href='./llava_llama3_8b.html'>llava-llama3:8b</a> 
<a href='./llava_phi3_3_8b.html'>llava-phi3:3.8b</a> 
<a href='./bakllava_7b.html'>bakllava:7b</a> 
<a href='./moondream_1_8b.html'>moondream:1.8b</a> 
</span>
</header>
<p>Prompt: (<a href='./prompt.txt'>raw</a>) (<a href='./describe_this_image____docs_logo_logo_250_jpg.prompt.yaml'>yaml</a>)
  words:4  bytes:46<br />
<textarea readonly rows='1'>Describe this image: ./docs/logo/logo-250.jpg</textarea>
</p>
<div class='box'><a target='image' href='logo-250.jpg'><img src='logo-250.jpg' alt='./docs/logo/logo-250.jpg' width='250' /></a></div><br /><p>Output: llava:7b (<a href='./llava_7b.output.txt'>raw</a>)<br />
<textarea readonly rows='4'> This is an illustration of three alpacas standing in front of a server rack. The alpaca on the left is looking to its right, while the two on the right are looking forward. All three have a pinkish color with darker ears and faces. They are inside what appears to be a data center or IT room, as indicated by the server racks filled with hardware in the background. The alpacas are anthropomorphized, standing upright like humans, and their expressions are not typical for animals; they seem to mimic human emotions or postures. There is no text present in the image. </textarea>
</p>
<div class='box'><table>
<tr><td class='left' colspan='2'>Stats (<a href='./llava_7b.stats.txt'>raw</a>)</td></tr>
<tr><td class='left'>words</td><td>101</td></tr>
<tr><td class='left'>bytes</td><td>571</td></tr>
<tr><td class='left'>total duration</td><td>11.440831208s</td></tr>
<tr><td class='left'>load duration</td><td>14.90275ms</td></tr>
<tr><td class='left'>prompt eval count</td><td>592 token(s)</td></tr>
<tr><td class='left'>prompt eval duration</td><td>3.971998958s</td></tr>
<tr><td class='left'>prompt eval rate</td><td>149.04 tokens/s</td></tr>
<tr><td class='left'>eval count</td><td>131 token(s)</td></tr>
<tr><td class='left'>eval duration</td><td>7.451343s</td></tr>
<tr><td class='left'>eval rate</td><td>17.58 tokens/s</td></tr>
</table></div>
<div class='box'><table>
<tr><td class='left' colspan='2'>Model (<a href='./llava_7b.info.txt'>raw</a>)</td></tr>
<tr><td class='left'>name</td><td class='left'><a target='ollama' href='https://ollama.com/library/llava:7b'>llava:7b</a></td></tr>
<tr><td class='left'>architecture</td><td class='left'>llama</td></tr>
<tr><td class='left'>size</td><td class='left'>7.0 GB</td></tr>
<tr><td class='left'>parameters</td><td class='left'>7.2B</td></tr>
<tr><td class='left'>context length</td><td class='left'>32768</td></tr>
<tr><td class='left'>embedding length</td><td  class='left'>4096</td></tr>
<tr><td class='left'>quantization</td><td class='left'>Q4_0</td></tr>
<tr><td class='left'>capabilities</td><td class='left'>completion<br />vision<br /></td>
</table></div>
<div class='box'><table>
<tr><td class='left' colspan='2'>System</td></tr>
<tr><td class='left'>ollama proc</td><td class='left'>100% GPU</td></tr>
<tr><td class='left'>ollama version</td><td class='left'>0.9.0</td></tr>
<tr><td class='left'>sys arch</td><td class='left'>arm64</td></tr>
<tr><td class='left'>sys processor</td><td class='left'>arm</td></tr>
<tr><td class='left'>sys memory</td><td class='left'>15G + 1006M</td></tr>
<tr><td class='left'>sys OS</td><td class='left'>Darwin 24.5.0</td></tr>
</table></div>
<br /><br />
<footer>
<p><a href='../index.html'>ollama-multirun</a>: <a href='./index.html'>describe_this_image____docs_logo_logo_250_jpg</a>: <b>llava:7b</b>: 20250615-224247</p>
<p>page created:   2025-06-15 22:45:28</p>
<p>Generated with: <a target='ollama-multirun' href='https://github.com/attogram/ollama-multirun'>ollama-multirun</a> v5.1</p>
</footer></body></html>
