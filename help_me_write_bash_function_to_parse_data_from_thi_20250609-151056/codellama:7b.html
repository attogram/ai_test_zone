  <!DOCTYPE html>
  <html lang="en">
  <head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <style>
    a:hover { background-color: yellow; color: black; }
    body { font-family: monospace; }
    header, footer { background-color: #f0f0f0; padding: 10px; }
    li { margin: 5px; }
    table, td, th { border-collapse: collapse; }
    td, th { border: 1px solid #cccccc; padding: 5px; text-align: right; }
    tr:hover { background-color: lightyellow; color: black; }
    textarea { border: 1px solid #cccccc; white-space: pre-wrap; width: 90%; }
    .box { display: inline-block; margin: 3px; padding: 2px; vertical-align: top; }
    .left { text-align: left; }
    .menu { font-size: small; }
  </style>
<title>ollama-multirun: codellama:7b</title></head><body>
<header><a href='../index.html'>ollama-multirun</a>: <a href='./index.html'>help_me_write_bash_function_to_parse_data_from_thi</a>: <b>codellama:7b</b>: 20250609-151056<br /><br />
<span class='menu'>
<a href='models.html'>models</a>: 
<a href='./qwen2.5-coder:7b.html'>qwen2.5-coder:7b</a> 
<b>codellama:7b</b> 
<a href='./starcoder:7b.html'>starcoder:7b</a> 
<a href='./stable-code:3b.html'>stable-code:3b</a> 
<a href='./deepcoder:1.5b.html'>deepcoder:1.5b</a> 
<a href='./dolphin3:8b.html'>dolphin3:8b</a> 
<a href='./gemma:2b.html'>gemma:2b</a> 
<a href='./granite3.3:2b.html'>granite3.3:2b</a> 
<a href='./mistral:7b.html'>mistral:7b</a> 
<a href='./qwen3:1.7b.html'>qwen3:1.7b</a> 
</span>
</header>
<p>Prompt: (<a href='./prompt.txt'>raw</a>) (<a href='./help_me_write_bash_function_to_parse_data_from_thi.prompt.yaml'>yaml</a>)
  words:174  bytes:1514<br />
<textarea readonly rows='10'>Help me write bash function to parse data from this model.info.txt file:

- in the Model section, each line is in format &quot;name    value&quot;
  - for each line, save as bash variable: name=&quot;$value&quot; 
  - example:  line &quot;architecture        granite&quot; parses to: architecture=&quot;granite&quot;
 
- in the Capabilities section, each line is a single keyword
  - save all the keywords into a bash array

- in the System section, single or multiple lines showing the system prompt
  - save as var $systemPrompt

- In the Parameters section (has same format as Model section)
  - only save the &#39;temperature&#39; value

- Allow that any section or any line may be missing

example model.info.txt contents:

  Model
    architecture        granite    
    parameters          2.5B       
    context length      16384      
    embedding length    2048       
    quantization        Q4_K_M     

  Capabilities
    completion    
    tools         
    vision        

  Projector
    architecture        clip       
    parameters          441.86M    
    embedding length    1152       
    dimensions          0          

  Parameters
    num_ctx        16384    
    temperature    0        

  System
    A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful,    
      detailed, and polite answers to the user&#39;s questions.                                                   

  License
    Apache License               
    Version 2.0, January 2004    
    ...                          </textarea>
</p>
<p>Output: codellama:7b (<a href='./codellama:7b.txt'>raw</a>)<br />
<textarea readonly rows='25'>
Here is a bash function that can parse data from a model.info.txt file with the given format:
```bash
function parse_model_info {
    local file=&quot;$1&quot; # name of the file to be parsed

    # declare arrays for each section
    local model=( )
    local capabilities=( )
    local system_prompt=&quot;&quot;
    local parameters=( )

    # read each line of the file and process it accordingly
    while IFS= read -r line; do
        if [[ $line == &quot;Model&quot;* ]]; then
            # parse model section
            local name=&quot;${line#*\ }&quot;
            local value=&quot;${line#* }&quot;
            model+=( &quot;$name=$value&quot; )
        elif [[ $line == &quot;Capabilities&quot;* ]]; then
            # parse capabilities section
            local keyword=&quot;${line#*\ }&quot;
            capabilities+=(&quot;$keyword&quot;)
        elif [[ $line == &quot;System&quot;* ]]; then
            # parse system prompt section
            system_prompt+=&quot;$line&quot;
        elif [[ $line == &quot;Parameters&quot;* ]]; then
            # parse parameters section
            local name=&quot;${line#*\ }&quot;
            if [[ $name == &quot;temperature&quot; ]]; then
                local value=&quot;${line#* }&quot;
                parameters+=( &quot;$name=$value&quot; )
            fi
        else
            # ignore other sections
            continue
        fi
    done &lt; &quot;$file&quot;

    # set bash variables for each parsed section
    printf -v &quot;architecture&quot; &quot;%s&quot; &quot;${model[0]#*=}&quot;
    printf -v &quot;parameters&quot; &quot;%s&quot; &quot;${parameters[0]#*=}&quot;
    printf -v &quot;system_prompt&quot; &quot;%s&quot; &quot;$system_prompt&quot;

    # return the parsed data as an array
    local -a result=( $architecture $parameters $system_prompt )
    echo &quot;${result[@]}&quot;
}
```
You can call this function by passing the name of the file to be parsed as an argument:
```bash
parse_model_info &quot;path/to/model.info.txt&quot;
```
This will set bash variables for each section and return the parsed data as an array.
Note that this function assumes that the format of the model.info.txt file is consistent with the given example, and it may need to be modified if the file has a different structure or layout.</textarea>
</p>
<div class='box'><table>
<tr><td class='left' colspan='2'>Stats (<a href='./codellama:7b.stats.txt'>raw</a>)</td></tr>
<tr><td class='left'>words</td><td>265</td></tr>
<tr><td class='left'>bytes</td><td>2035</td></tr>
<tr><td class='left'>total duration</td><td>32.984281709s</td></tr>
<tr><td class='left'>load duration</td><td>2.83472875s</td></tr>
<tr><td class='left'>prompt eval count</td><td>403 token(s)</td></tr>
<tr><td class='left'>prompt eval duration</td><td>2.933331875s</td></tr>
<tr><td class='left'>prompt eval rate</td><td>137.39 tokens/s</td></tr>
<tr><td class='left'>eval count</td><td>534 token(s)</td></tr>
<tr><td class='left'>eval duration</td><td>27.214457708s</td></tr>
<tr><td class='left'>eval rate</td><td>19.62 tokens/s</td></tr>
</table></div>
<div class='box'><table>
<tr><td class='left' colspan='2'>Model (<a href='./codellama:7b.info.txt'>raw</a>)</td></tr>
<tr><td class='left'>name</td><td class='left'><a target='ollama' href='https://ollama.com/library/codellama:7b'>codellama:7b</a></td></tr>
<tr><td class='left'>architecture</td><td class='left'>llama</td></tr>
<tr><td class='left'>size</td><td class='left'>9.4 GB</td></tr>
<tr><td class='left'>parameters</td><td class='left'>6.7B</td></tr>
<tr><td class='left'>context length</td><td class='left'>16384</td></tr>
<tr><td class='left'>embedding length</td><td  class='left'>4096</td></tr>
<tr><td class='left'>quantization</td><td class='left'>Q4_0</td></tr>
<tr><td class='left'>capabilities</td><td class='left'>completion<br /></td>
</table></div>
<div class='box'><table>
<tr><td class='left' colspan='2'>System</td></tr>
<tr><td class='left'>ollama proc</td><td class='left'>100% GPU</td></tr>
<tr><td class='left'>ollama version</td><td class='left'>0.9.0</td></tr>
<tr><td class='left'>sys arch</td><td class='left'>arm64</td></tr>
<tr><td class='left'>sys processor</td><td class='left'>arm</td></tr>
<tr><td class='left'>sys memory</td><td class='left'>11G + 812M</td></tr>
<tr><td class='left'>sys OS</td><td class='left'>Darwin 24.5.0</td></tr>
</table></div>
<br /><br />
<footer>
<p>page created:   2025-06-09 15:12:19</p>
<p>Generated with: <a target='ollama-multirun' href='https://github.com/attogram/ollama-multirun'>ollama-multirun</a> v4.2</p>
</footer></body></html>
